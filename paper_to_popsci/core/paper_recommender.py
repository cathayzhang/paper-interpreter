"""
è®ºæ–‡å…³ç³»æ¢ç´¢ä¸æ™ºèƒ½æ¨èæ¨¡å—
æ•´åˆ Semantic Scholar APIã€OpenAlex APIã€arXiv API å’Œæœ¬åœ°å…³é”®è¯åŒ¹é…
æ— éœ€ API Key ä¹Ÿèƒ½ä½¿ç”¨åŸºç¡€åŠŸèƒ½
"""
import requests
import re
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from difflib import SequenceMatcher
import logging

from .logger import logger


@dataclass
class RelatedPaper:
    """ç›¸å…³è®ºæ–‡æ•°æ®ç±»"""
    title: str
    authors: List[str]
    year: int
    abstract: str
    url: str
    pdf_url: Optional[str]
    citation_count: int
    source: str  # æ¨èæ¥æºè¯´æ˜
    relevance_score: float = 0.0  # ç›¸å…³åº¦åˆ†æ•° (0-1)
    reason: str = ""  # æ¨èç†ç”±


class PaperRecommender:
    """
    è®ºæ–‡æ™ºèƒ½æ¨èå™¨ - æ— éœ€ API Key ä¹Ÿèƒ½ä½¿ç”¨

    ä½¿ç”¨ç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. Semantic Scholar APIï¼ˆæ— éœ€ Keyï¼Œæœ‰å…±äº«é€Ÿç‡é™åˆ¶ï¼‰
    2. OpenAlex APIï¼ˆå…è´¹ï¼Œæ¨èæä¾› emailï¼‰
    3. arXiv APIï¼ˆå®Œå…¨å…è´¹ï¼Œæ— éœ€æ³¨å†Œï¼‰
    4. æœ¬åœ°å…³é”®è¯åŒ¹é…ï¼ˆç¦»çº¿å¯ç”¨ï¼‰
    """

    def __init__(self, ss_api_key: str = None, openalex_email: str = None):
        self.ss_api_key = ss_api_key
        self.openalex_email = openalex_email
        self.ss_base_url = "https://api.semanticscholar.org"
        self.ss_headers = {}
        if ss_api_key:
            self.ss_headers["x-api-key"] = ss_api_key

    def get_recommendations(
        self,
        paper_title: str,
        paper_abstract: str = "",
        arxiv_id: str = None,
        doi: str = None,
        semantic_scholar_id: str = None,
        limit: int = 10
    ) -> Dict[str, List[RelatedPaper]]:
        """
        è·å–è®ºæ–‡æ¨è - æ— éœ€ API Key ä¹Ÿèƒ½ä½¿ç”¨

        Returns:
            {
                "semantic_scholar": [...],  # API æ™ºèƒ½æ¨è
                "citations": [...],          # å¼•ç”¨ç½‘ç»œç›¸å…³
                "similar_topics": [...]      # ç›¸ä¼¼ä¸»é¢˜
            }
        """
        result = {
            "semantic_scholar": [],
            "citations": [],
            "similar_topics": []
        }

        # 1. å°è¯•è·å– Semantic Scholar æ¨èï¼ˆæ— éœ€ Key ä¹Ÿèƒ½ä½¿ç”¨ï¼‰
        paper_id = self._resolve_paper_id(arxiv_id, doi, semantic_scholar_id)
        if paper_id:
            try:
                result["semantic_scholar"] = self._get_ss_recommendations(paper_id, limit)
                result["citations"] = self._get_citation_network(paper_id, limit // 2)
            except Exception as e:
                logger.warning(f"Semantic Scholar API å¤±è´¥ï¼Œå°è¯•å¤‡é€‰æ–¹æ¡ˆ: {e}")

        # 2. å¦‚æœ Semantic Scholar å¤±è´¥ï¼Œå°è¯• arXiv APIï¼ˆå®Œå…¨å…è´¹ï¼‰
        if not result["semantic_scholar"] and arxiv_id:
            try:
                result["semantic_scholar"] = self._get_arxiv_recommendations(
                    arxiv_id, paper_title, limit
                )
            except Exception as e:
                logger.warning(f"arXiv API å¤±è´¥: {e}")

        # 3. å¦‚æœéƒ½å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°å…³é”®è¯åŒ¹é…
        if not result["semantic_scholar"] and paper_title:
            result["similar_topics"] = self._get_local_keyword_recommendations(
                paper_title, paper_abstract, limit
            )

        return result

    def _resolve_paper_id(self, arxiv_id: str, doi: str, ss_id: str) -> Optional[str]:
        """è§£æè®ºæ–‡ ID æ ¼å¼"""
        if ss_id:
            return ss_id
        if arxiv_id:
            return f"arxiv:{arxiv_id.replace('arxiv:', '')}"
        if doi:
            return f"doi:{doi}"
        return None

    def _get_ss_recommendations(self, paper_id: str, limit: int) -> List[RelatedPaper]:
        """
        è·å– Semantic Scholar æ¨è
        æ³¨æ„ï¼šæ— éœ€ API Key ä¹Ÿèƒ½ä½¿ç”¨ï¼Œåªæ˜¯æœ‰é€Ÿç‡é™åˆ¶
        """
        endpoint = f"{self.ss_base_url}/recommendations/v1/papers/forpaper/{paper_id}"

        params = {
            "limit": limit,
            "fields": "paperId,title,authors,year,citationCount,referenceCount,"
                      "abstract,url,openAccessPdf,publicationDate,fieldsOfStudy",
        }

        try:
            response = requests.get(
                endpoint,
                params=params,
                headers=self.ss_headers,
                timeout=10  # è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            )

            # å¤„ç†é€Ÿç‡é™åˆ¶
            if response.status_code == 429:
                logger.warning("Semantic Scholar é€Ÿç‡é™åˆ¶ï¼Œå°†ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ")
                return []

            response.raise_for_status()
            data = response.json()
            recommendations = data.get("recommendedPapers", [])

            results = []
            for paper in recommendations:
                authors = [a.get("name") for a in paper.get("authors", [])[:3]]
                if len(paper.get("authors", [])) > 3:
                    authors.append("et al.")

                results.append(RelatedPaper(
                    title=paper.get("title", ""),
                    authors=authors,
                    year=paper.get("year", 0),
                    abstract=self._truncate_abstract(paper.get("abstract", "")),
                    url=paper.get("url", ""),
                    pdf_url=paper.get("openAccessPdf", {}).get("url") if paper.get("openAccessPdf") else None,
                    citation_count=paper.get("citationCount", 0),
                    source="Semantic Scholar æ™ºèƒ½æ¨è",
                    relevance_score=0.9,
                    reason="åŸºäºå¼•ç”¨ç½‘ç»œå’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ™ºèƒ½æ¨è"
                ))

            logger.info(f"Semantic Scholar æ¨è: {len(results)} ç¯‡")
            return results

        except requests.exceptions.Timeout:
            logger.warning("Semantic Scholar API è¶…æ—¶")
            return []
        except Exception as e:
            logger.error(f"Semantic Scholar API è°ƒç”¨å¤±è´¥: {e}")
            return []

    def _get_citation_network(self, paper_id: str, limit: int) -> List[RelatedPaper]:
        """è·å–å¼•ç”¨ç½‘ç»œ"""
        results = []

        try:
            details_endpoint = f"{self.ss_base_url}/graph/v1/paper/{paper_id}"
            params = {"fields": "citations,references,title"}

            response = requests.get(
                details_endpoint,
                params=params,
                headers=self.ss_headers,
                timeout=10
            )

            if response.status_code == 429:
                return []

            response.raise_for_status()
            data = response.json()

            # å¼•ç”¨è¿™ç¯‡è®ºæ–‡çš„
            citations = data.get("citations", [])[:limit]
            for cite in citations:
                results.append(RelatedPaper(
                    title=cite.get("title", ""),
                    authors=["æŸ¥çœ‹è¯¦æƒ…"],
                    year=cite.get("year", 0),
                    abstract="",
                    url=f"https://www.semanticscholar.org/paper/{cite.get('paperId')}",
                    pdf_url=None,
                    citation_count=cite.get("citationCount", 0),
                    source="å¼•ç”¨è¯¥è®ºæ–‡",
                    relevance_score=0.8,
                    reason="åç»­ç ”ç©¶å¼•ç”¨äº†æœ¬æ–‡"
                ))

            # è¿™ç¯‡è®ºæ–‡å¼•ç”¨çš„
            references = data.get("references", [])[:limit]
            for ref in references:
                results.append(RelatedPaper(
                    title=ref.get("title", ""),
                    authors=["æŸ¥çœ‹è¯¦æƒ…"],
                    year=ref.get("year", 0),
                    abstract="",
                    url=f"https://www.semanticscholar.org/paper/{ref.get('paperId')}",
                    pdf_url=None,
                    citation_count=ref.get("citationCount", 0),
                    source="å‚è€ƒæ–‡çŒ®",
                    relevance_score=0.75,
                    reason="æœ¬æ–‡å¼•ç”¨çš„å‰æœŸå·¥ä½œ"
                ))

            return results

        except Exception as e:
            logger.warning(f"è·å–å¼•ç”¨ç½‘ç»œå¤±è´¥: {e}")
            return []

    def _get_arxiv_recommendations(self, arxiv_id: str, title: str, limit: int) -> List[RelatedPaper]:
        """
        ä½¿ç”¨ arXiv API è·å–æ¨èï¼ˆå®Œå…¨å…è´¹ï¼Œæ— éœ€æ³¨å†Œï¼‰
        ç­–ç•¥ï¼šåŸºäºæ ‡é¢˜å…³é”®è¯æœç´¢ç›¸å…³è®ºæ–‡
        """
        try:
            # æå–å…³é”®è¯ï¼ˆç®€å•çš„ TF æ–¹æ³•ï¼‰
            keywords = self._extract_keywords(title)
            if not keywords:
                return []

            # ä½¿ç”¨ arXiv API æœç´¢
            query = " OR ".join(keywords[:3])  # ä½¿ç”¨å‰3ä¸ªå…³é”®è¯
            arxiv_endpoint = "http://export.arxiv.org/api/query"

            params = {
                "search_query": f"all:{query}",
                "start": 0,
                "max_results": limit,
                "sortBy": "relevance",
                "sortOrder": "descending"
            }

            response = requests.get(arxiv_endpoint, params=params, timeout=15)
            response.raise_for_status()

            # è§£æ arXiv Atom feed
            import xml.etree.ElementTree as ET

            root = ET.fromstring(response.content)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}

            results = []
            for entry in root.findall('atom:entry', ns)[:limit]:
                entry_id = entry.find('atom:id', ns).text
                entry_title = entry.find('atom:title', ns).text
                entry_summary = entry.find('atom:summary', ns).text
                entry_published = entry.find('atom:published', ns).text

                # æå–å¹´ä»½
                year = int(entry_published[:4]) if entry_published else 0

                # æå–ä½œè€…
                authors = [author.find('atom:name', ns).text
                          for author in entry.findall('atom:author', ns)[:3]]

                # æå– arXiv ID
                arxiv_match = re.search(r'arXiv:(\d+\.\d+)', entry_id)
                arxiv_num = arxiv_match.group(1) if arxiv_match else ""

                # æ’é™¤è‡ªèº«
                if arxiv_id.replace('arxiv:', '') in entry_id:
                    continue

                # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
                similarity = SequenceMatcher(None, title.lower(), entry_title.lower()).ratio()

                results.append(RelatedPaper(
                    title=entry_title.replace('\n', ' ').strip(),
                    authors=authors,
                    year=year,
                    abstract=self._truncate_abstract(entry_summary),
                    url=f"https://arxiv.org/abs/{arxiv_num}" if arxiv_num else entry_id,
                    pdf_url=f"https://arxiv.org/pdf/{arxiv_num}.pdf" if arxiv_num else None,
                    citation_count=0,  # arXiv ä¸æä¾›å¼•ç”¨æ•°
                    source="arXiv ç›¸å…³æ¨è",
                    relevance_score=similarity,
                    reason="åŸºäºæ ‡é¢˜å…³é”®è¯çš„ç›¸å…³æ€§åŒ¹é…"
                ))

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x.relevance_score, reverse=True)

            logger.info(f"arXiv æ¨è: {len(results)} ç¯‡")
            return results[:limit]

        except Exception as e:
            logger.error(f"arXiv API è°ƒç”¨å¤±è´¥: {e}")
            return []

    def _get_local_keyword_recommendations(
        self,
        title: str,
        abstract: str,
        limit: int
    ) -> List[RelatedPaper]:
        """
        æœ¬åœ°å…³é”®è¯æ¨èï¼ˆç¦»çº¿å¯ç”¨ï¼Œæ— éœ€ç½‘ç»œï¼‰
        ç”Ÿæˆæœç´¢é“¾æ¥ä¾›ç”¨æˆ·è‡ªè¡ŒæŸ¥æ‰¾
        """
        keywords = self._extract_keywords(title + " " + abstract)

        results = []

        # ç”Ÿæˆ Semantic Scholar æœç´¢é“¾æ¥
        if keywords:
            query = "+".join(keywords[:3])
            results.append(RelatedPaper(
                title="åœ¨ Semantic Scholar ä¸Šæœç´¢ç›¸å…³è®ºæ–‡",
                authors=[],
                year=0,
                abstract=f"å…³é”®è¯: {', '.join(keywords[:5])}",
                url=f"https://www.semanticscholar.org/search?q={query}&sort=relevance",
                pdf_url=None,
                citation_count=0,
                source="å…³é”®è¯æœç´¢",
                relevance_score=1.0,
                reason="åŸºäºè®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦æå–çš„å…³é”®è¯"
            ))

        # ç”Ÿæˆ Google Scholar æœç´¢é“¾æ¥
        if keywords:
            query = "+".join(keywords[:3])
            results.append(RelatedPaper(
                title="åœ¨ Google Scholar ä¸Šæœç´¢ç›¸å…³è®ºæ–‡",
                authors=[],
                year=0,
                abstract="Google Scholar æä¾›æ›´å¹¿æ³›çš„å­¦æœ¯æ–‡çŒ®æœç´¢",
                url=f"https://scholar.google.com/scholar?q={query}",
                pdf_url=None,
                citation_count=0,
                source="å…³é”®è¯æœç´¢",
                relevance_score=0.95,
                reason="Google Scholar åŒ…å«æ›´å…¨é¢çš„å­¦æœ¯æ–‡çŒ®"
            ))

        # ç”Ÿæˆ arXiv æœç´¢é“¾æ¥
        if keywords:
            query = "+OR+".join(keywords[:3])
            results.append(RelatedPaper(
                title="åœ¨ arXiv ä¸Šæœç´¢ç›¸å…³é¢„å°æœ¬",
                authors=[],
                year=0,
                abstract="arXiv æ˜¯è®¡ç®—æœºç§‘å­¦ã€ç‰©ç†ã€æ•°å­¦ç­‰é¢†åŸŸçš„é‡è¦é¢„å°æœ¬åº“",
                url=f"https://arxiv.org/search/?query={query}&searchtype=all",
                pdf_url=None,
                citation_count=0,
                source="å…³é”®è¯æœç´¢",
                relevance_score=0.9,
                reason="arXiv åŒ…å«æœ€æ–°ç ”ç©¶è¿›å±•"
            ))

        logger.info(f"æœ¬åœ°å…³é”®è¯æ¨è: {len(results)} ä¸ªæœç´¢é“¾æ¥")
        return results[:limit]

    def _extract_keywords(self, text: str, top_n: int = 5) -> List[str]:
        """
        ç®€å•çš„å…³é”®è¯æå–ï¼ˆåŸºäºè¯é¢‘ï¼Œæ— éœ€ NLP åº“ï¼‰
        """
        if not text:
            return []

        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text.split()

        # åœç”¨è¯åˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
        stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be',
            'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will',
            'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this',
            'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
            'using', 'based', 'via', 'through', 'over', 'under', 'between', 'among',
            'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself',
            'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',
            'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs',
            'themselves', 'what', 'which', 'who', 'whom', 'whose', 'where', 'when',
            'why', 'how', 'all', 'each', 'few', 'more', 'most', 'other', 'some',
            'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',
            'very', 'just', 'now', 'also', 'new', 'novel', 'proposed', 'approach',
            'method', 'methods', 'algorithm', 'algorithms', 'model', 'models',
            'system', 'systems', 'framework', 'frameworks', 'technique', 'techniques'
        }

        # ç»Ÿè®¡è¯é¢‘ï¼ˆåªä¿ç•™é•¿åº¦ >= 4 çš„è¯ï¼‰
        word_freq = {}
        for word in words:
            if len(word) >= 4 and word not in stopwords and word.isalpha():
                word_freq[word] = word_freq.get(word, 0) + 1

        # è¿”å›æœ€å¸¸è§çš„è¯
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:top_n]]

    def _truncate_abstract(self, abstract: str, max_length: int = 200) -> str:
        """æˆªæ–­æ‘˜è¦"""
        if not abstract:
            return ""
        abstract = abstract.replace('\n', ' ').strip()
        if len(abstract) <= max_length:
            return abstract
        return abstract[:max_length].rsplit(" ", 1)[0] + "..."

    def format_for_article(self, recommendations: Dict[str, List[RelatedPaper]]) -> str:
        """å°†æ¨èç»“æœæ ¼å¼åŒ–ä¸ºæ–‡ç«  markdown æ ¼å¼"""
        lines = []
        # æ³¨æ„ï¼šç« èŠ‚æ ‡é¢˜ç”± renderer å¤„ç†ï¼Œè¿™é‡Œä¸éœ€è¦æ·»åŠ  ## æ ‡é¢˜

        has_recommendations = any(
            recommendations.get(key) for key in ["semantic_scholar", "citations", "similar_topics"]
        )

        if has_recommendations:
            lines.append("åŸºäºå­¦æœ¯è®ºæ–‡å¼•ç”¨ç½‘ç»œå’Œè¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æï¼Œä¸ºæ‚¨æ¨èä»¥ä¸‹ç›¸å…³ç ”ç©¶ï¼š")
        else:
            lines.append("æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•è·å–è‡ªåŠ¨æ¨èã€‚æ‚¨å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹å¼æ¢ç´¢ç›¸å…³ç ”ç©¶ï¼š")
        lines.append("")

        # 1. Semantic Scholar / arXiv æ¨è
        ss_recs = recommendations.get("semantic_scholar", [])
        if ss_recs:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœç´¢é“¾æ¥ç±»å‹
            if ss_recs[0].source == "å…³é”®è¯æœç´¢":
                lines.append("### ğŸ” ç›¸å…³è®ºæ–‡æœç´¢")
            else:
                lines.append("### ğŸ”¬ ç›¸å…³è®ºæ–‡æ¨è")
            lines.append("")

            for i, paper in enumerate(ss_recs[:5], 1):
                if paper.source == "å…³é”®è¯æœç´¢":
                    # æœç´¢é“¾æ¥ç±»å‹
                    lines.append(f"**{i}. [{paper.title}]({paper.url})**")
                    if paper.abstract:
                        lines.append(f"- **å…³é”®è¯**: {paper.abstract}")
                    if paper.reason:
                        lines.append(f"- **è¯´æ˜**: {paper.reason}")
                else:
                    # å®é™…è®ºæ–‡æ¨è
                    lines.append(f"**{i}. {paper.title}** ({paper.year})")
                    lines.append("")
                    if paper.authors:
                        lines.append(f"**ä½œè€…**: {', '.join(paper.authors)}")
                        lines.append("")
                    if paper.citation_count:
                        lines.append(f"**è¢«å¼•æ¬¡æ•°**: {paper.citation_count}")
                        lines.append("")
                    if paper.abstract:
                        lines.append(f"**ç®€ä»‹**: {paper.abstract}")
                        lines.append("")
                    if paper.url:
                        lines.append(f"**é“¾æ¥**: [ç‚¹å‡»æŸ¥çœ‹è¯¦æƒ…]({paper.url})")
                        lines.append("")
                        # æ·»åŠ è®ºæ–‡è§£è¯»æŒ‰é’®é“¾æ¥
                        lines.append(f"**[ğŸ“„ ä¸€é”®è§£è¯»è¿™ç¯‡è®ºæ–‡]({paper.url})**")
                        lines.append("")
                    if paper.pdf_url:
                        lines.append(f"**PDF**: [å…è´¹ä¸‹è½½]({paper.pdf_url})")
                        lines.append("")
                    if paper.reason:
                        lines.append(f"**æ¨èç†ç”±**: {paper.reason}")
                        lines.append("")
                lines.append("")

        # 2. å¼•ç”¨ç½‘ç»œ
        citations = recommendations.get("citations", [])
        if citations:
            lines.append("### ğŸ“š å¼•ç”¨ç½‘ç»œ")
            lines.append("")

            citing = [c for c in citations if c.source == "å¼•ç”¨è¯¥è®ºæ–‡"]
            referenced = [c for c in citations if c.source == "å‚è€ƒæ–‡çŒ®"]

            if citing:
                lines.append("**å¼•ç”¨è¯¥è®ºæ–‡çš„ç ”ç©¶ï¼š**")
                for paper in citing[:3]:
                    lines.append(f"- [{paper.title}]({paper.url}) ({paper.year})")
                lines.append("")

            if referenced:
                lines.append("**è¯¥è®ºæ–‡å¼•ç”¨çš„å‰æœŸå·¥ä½œï¼š**")
                for paper in referenced[:3]:
                    lines.append(f"- [{paper.title}]({paper.url}) ({paper.year})")
                lines.append("")

        # 3. æ‰‹åŠ¨æ¢ç´¢å»ºè®®
        if not has_recommendations:
            lines.append("### ğŸ’¡ æ‰‹åŠ¨æ¢ç´¢å»ºè®®")
            lines.append("")
            lines.append("1. **Semantic Scholar**: è®¿é—® semanticscholar.org æœç´¢è®ºæ–‡æ ‡é¢˜")
            lines.append("2. **Google Scholar**: ä½¿ç”¨ scholar.google.com æŸ¥æ‰¾å¼•ç”¨ç½‘ç»œ")
            lines.append("3. **arXiv**: å¦‚æœæ˜¯è®¡ç®—æœºç§‘å­¦è®ºæ–‡ï¼Œåœ¨ arxiv.org æŸ¥æ‰¾ç›¸å…³é¢„å°æœ¬")
            lines.append("4. **æŸ¥çœ‹å‚è€ƒæ–‡çŒ®**: é˜…è¯»åŸæ–‡çš„å‚è€ƒæ–‡çŒ®ç« èŠ‚ï¼Œäº†è§£ç ”ç©¶èƒŒæ™¯")
            lines.append("")

        lines.append("---")
        lines.append("")

        return "\n".join(lines)
