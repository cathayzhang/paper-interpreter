#!/usr/bin/env python3
"""
Paper to PopSci - å‘½ä»¤è¡Œå…¥å£

ç”¨æ³•:
    python -m paper_to_popsci.cli <è®ºæ–‡é“¾æ¥>

ç¤ºä¾‹:
    python -m paper_to_popsci.cli https://arxiv.org/abs/2312.00752
"""
import sys
import time
import json
import shutil
from pathlib import Path
from datetime import datetime
import argparse
from typing import Optional

from .core.config import Config
from .core.logger import logger
from .core.downloader import PaperDownloader
from .core.extractor import PDFExtractor
from .core.analyzer import ContentAnalyzer
from .core.illustrator import IllustrationGenerator
from .core.writer import ArticleWriter
from .core.renderer import HTMLRenderer, PDFExporter


def sanitize_filename(name: str) -> str:
    """æ¸…ç†æ–‡ä»¶å"""
    # ç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨å­—ç¬¦
    unsafe_chars = '<>:"/\\|?*'
    for char in unsafe_chars:
        name = name.replace(char, '_')
    # é™åˆ¶é•¿åº¦
    if len(name) > 100:
        name = name[:100]
    return name.strip()


def process_paper(url: str, output_dir: Optional[str] = None) -> dict:
    """
    å¤„ç†è®ºæ–‡çš„ä¸»æµç¨‹

    Args:
        url: è®ºæ–‡é“¾æ¥
        output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰

    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    start_time = time.time()

    logger.info("=" * 60)
    logger.info(f"å¼€å§‹å¤„ç†è®ºæ–‡: {url}")
    logger.info("=" * 60)

    # 1. ä¸‹è½½è®ºæ–‡
    logger.info("\n[1/7] ä¸‹è½½è®ºæ–‡...")
    downloader = PaperDownloader()
    try:
        pdf_path, metadata = downloader.download(url)
        logger.info(f"âœ“ è®ºæ–‡ä¸‹è½½æˆåŠŸ: {pdf_path}")
    except Exception as e:
        logger.error(f"âœ— è®ºæ–‡ä¸‹è½½å¤±è´¥: {e}")
        return {"success": False, "error": f"ä¸‹è½½å¤±è´¥: {e}"}

    # 2. æå–å†…å®¹
    logger.info("\n[2/7] æå– PDF å†…å®¹...")
    extractor = PDFExtractor()
    try:
        paper_content = extractor.extract(pdf_path, metadata)
        logger.info(f"âœ“ å†…å®¹æå–æˆåŠŸ: {len(paper_content.sections)} ä¸ªç« èŠ‚")
        logger.info(f"  æ ‡é¢˜: {paper_content.title[:60] if paper_content.title else 'N/A'}...")
        logger.info(f"  ä½œè€…: {', '.join(paper_content.authors[:3]) if paper_content.authors else 'N/A'}")
    except Exception as e:
        logger.error(f"âœ— å†…å®¹æå–å¤±è´¥: {e}")
        return {"success": False, "error": f"æå–å¤±è´¥: {e}"}

    # ç¡®å®šè¾“å‡ºç›®å½•
    if output_dir:
        work_dir = Path(output_dir)
    else:
        paper_title = sanitize_filename(paper_content.title or "untitled")
        work_dir = Path(Config.OUTPUT_DIR) / f"{paper_title}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    # å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
    if work_dir.exists():
        work_dir = Path(str(work_dir) + f"_{int(time.time())}")

    work_dir.mkdir(parents=True, exist_ok=True)
    assets_dir = work_dir / "assets" / "images"
    assets_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"  è¾“å‡ºç›®å½•: {work_dir}")

    # ä¿å­˜å…ƒæ•°æ®
    metadata_path = work_dir / "metadata.json"
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump({
            "source_url": url,
            "title": paper_content.title,
            "authors": paper_content.authors,
            "publication_date": paper_content.publication_date,
            "arxiv_id": paper_content.arxiv_id,
            "doi": paper_content.doi,
            "extraction_method": paper_content.extraction_method,
        }, f, ensure_ascii=False, indent=2)

    # 3. åˆ†æå†…å®¹
    logger.info("\n[3/7] åˆ†æè®ºæ–‡å†…å®¹...")
    analyzer = ContentAnalyzer()
    try:
        analysis_result = analyzer.analyze(paper_content)
        outline = analysis_result["outline"]
        illustration_prompts = analysis_result["illustration_prompts"]
        logger.info(f"âœ“ åˆ†æå®Œæˆ: ç±»å‹={outline.get('article_type', 'N/A')}")
        logger.info(f"  æ ¸å¿ƒåˆ›æ–°: {outline.get('core_innovation', 'N/A')[:60]}...")
    except Exception as e:
        logger.warning(f"âš  å†…å®¹åˆ†æå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å¤§çº²")
        analysis_result = analyzer._get_default_outline(paper_content)
        outline = analysis_result["outline"]
        illustration_prompts = analysis_result["illustration_prompts"]

    # 4. ç”Ÿæˆé…å›¾
    logger.info("\n[4/7] ç”Ÿæˆé…å›¾...")
    illustrator = IllustrationGenerator()
    try:
        illustrations = illustrator.generate_all(illustration_prompts, assets_dir)
        success_count = sum(1 for ill in illustrations if ill.get("success"))
        logger.info(f"âœ“ é…å›¾ç”Ÿæˆå®Œæˆ: {success_count}/{len(illustrations)} å¼ æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš  é…å›¾ç”Ÿæˆå¤±è´¥: {e}")
        illustrations = []

    # 5. ç”Ÿæˆæ–‡ç« 
    logger.info("\n[5/7] ç”Ÿæˆæ–‡ç« ...")
    writer = ArticleWriter()
    try:
        article_sections = writer.write(paper_content, analysis_result, illustrations)
        word_count = sum(len(section.content) for section in article_sections)
        logger.info(f"âœ“ æ–‡ç« ç”Ÿæˆå®Œæˆ: {len(article_sections)} ä¸ªç« èŠ‚, {word_count} å­—")
    except Exception as e:
        logger.error(f"âœ— æ–‡ç« ç”Ÿæˆå¤±è´¥: {e}")
        return {"success": False, "error": f"å†™ä½œå¤±è´¥: {e}"}

    # 6. æ¸²æŸ“ HTML
    logger.info("\n[6/7] æ¸²æŸ“ HTML...")
    html_renderer = HTMLRenderer()
    try:
        html_path = work_dir / "article.html"
        html_renderer.render(article_sections, paper_content, html_path)
        logger.info(f"âœ“ HTML æ¸²æŸ“å®Œæˆ: {html_path}")
    except Exception as e:
        logger.error(f"âœ— HTML æ¸²æŸ“å¤±è´¥: {e}")
        return {"success": False, "error": f"æ¸²æŸ“å¤±è´¥: {e}"}

    # 7. å¯¼å‡º PDF
    logger.info("\n[7/7] å¯¼å‡º PDF...")
    pdf_exporter = PDFExporter()
    try:
        pdf_path = work_dir / "article.pdf"
        result_path = pdf_exporter.export(html_path, pdf_path)
        if result_path == pdf_path:
            logger.info(f"âœ“ PDF å¯¼å‡ºæˆåŠŸ: {pdf_path}")
        else:
            logger.warning(f"âš  PDF å¯¼å‡ºå¤±è´¥ï¼Œä¿ç•™ HTML: {result_path}")
    except Exception as e:
        logger.warning(f"âš  PDF å¯¼å‡ºå¤±è´¥: {e}")
        pdf_path = None

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    try:
        shutil.rmtree(pdf_path.parent / "assets" / "temp", ignore_errors=True)
    except:
        pass

    # ç»Ÿè®¡
    elapsed_time = time.time() - start_time

    result = {
        "success": True,
        "output_dir": str(work_dir),
        "files": {
            "html": str(html_path) if html_path.exists() else None,
            "pdf": str(pdf_path) if pdf_path and pdf_path.exists() else None,
            "metadata": str(metadata_path),
        },
        "statistics": {
            "illustrations_generated": sum(1 for ill in illustrations if ill.get("success")),
            "illustrations_total": len(illustrations),
            "article_sections": len(article_sections),
            "word_count": word_count,
            "elapsed_time": round(elapsed_time, 2),
        },
        "paper_info": {
            "title": paper_content.title,
            "authors": paper_content.authors,
            "publication_date": paper_content.publication_date,
        }
    }

    # è¾“å‡ºç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("âœ… è®ºæ–‡è§£è¯»å®Œæˆ!")
    logger.info("=" * 60)
    logger.info(f"\nğŸ“„ è®ºæ–‡: {paper_content.title or 'N/A'}")
    logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {work_dir}")
    logger.info(f"\nç”Ÿæˆæ–‡ä»¶:")
    logger.info(f"  â”œâ”€â”€ article.html")
    if pdf_path and pdf_path.exists():
        logger.info(f"  â”œâ”€â”€ article.pdf")
    logger.info(f"  â”œâ”€â”€ assets/images/ ({result['statistics']['illustrations_generated']} å¼ é…å›¾)")
    logger.info(f"  â””â”€â”€ metadata.json")
    logger.info(f"\nğŸ“Š ç»Ÿè®¡:")
    logger.info(f"  â€¢ æ–‡ç« å­—æ•°: {word_count} å­—")
    logger.info(f"  â€¢ å¤„ç†è€—æ—¶: {elapsed_time:.1f} ç§’")

    return result


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="Paper to PopSci - å°†å­¦æœ¯è®ºæ–‡è½¬æ¢ä¸ºé€šä¿—æ˜“æ‡‚çš„ç§‘æ™®æ–‡ç« ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  %(prog)s https://arxiv.org/abs/2312.00752
  %(prog)s https://doi.org/10.1145/276675.276685 -o ./output
        """
    )

    parser.add_argument(
        "url",
        help="è®ºæ–‡é“¾æ¥ (æ”¯æŒ arXiv, DOI, OpenReview, Semantic Scholar ç­‰)"
    )
    parser.add_argument(
        "-o", "--output",
        dest="output_dir",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ./paper_outputs/è®ºæ–‡æ ‡é¢˜_æ—¶é—´æˆ³/)"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—"
    )

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        import logging
        logging.getLogger("paper_to_popsci").setLevel(logging.DEBUG)

    # å¤„ç†è®ºæ–‡
    result = process_paper(args.url, args.output_dir)

    # è¿”å›çŠ¶æ€ç 
    if result["success"]:
        sys.exit(0)
    else:
        logger.error(f"\nå¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        sys.exit(1)


if __name__ == "__main__":
    main()
